{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a3b26-5377-4a6a-be71-8c2b372a48b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "058edbd2-17b2-462f-a106-b89b59845285",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Slide 1](./images/Slide2.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc40eb8c-5d3d-4309-bbf9-1b73ab0f9f65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Slide 2](./images/Slide3.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843946a-c04f-43b2-ab80-f21320ddbb1d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Slide 3](./images/Slide4.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505d26b8-dacd-4f5e-88fd-f1d689b959c5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Slide 4](./images/Slide5.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f616d941-9f96-41bc-bdf4-027a9c7c334a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Slide 5](./images/Slide6.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b30bf-0f55-479b-aceb-4608ebc751ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "![Slide 6](./images/Slide7.JPG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d949fe5-538d-46d5-b22f-c9f10c61a154",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "def ask_multiline_question(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask a question that can span multiple lines.\n",
    "    The user enters lines until a blank line is entered.\n",
    "    \"\"\"\n",
    "    print(query)\n",
    "    lines = []\n",
    "    while True:\n",
    "        line = input()\n",
    "        if not line.strip():  # Stop reading on an empty line\n",
    "            break\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "\n",
    "def ask_question(query: str) -> str:\n",
    "    \"\"\"Ask a single-line question and return the answer.\"\"\"\n",
    "    return input(query)\n",
    "\n",
    "\n",
    "def run():\n",
    "    # 1. SET UP REPORT TOPIC\n",
    "    # Get initial multi-line query from the user\n",
    "    initial_query = ask_multiline_question(\n",
    "        \"What would you like to research? (Enter blank line to finish)\"\n",
    "    )\n",
    "\n",
    "    # Get breadth parameter (with default of 4 if input is empty or invalid)\n",
    "    breadth_input = ask_question(\"Enter research breadth (recommended 2-10, default 4): \")\n",
    "    try:\n",
    "        breadth = int(breadth_input)\n",
    "    except ValueError:\n",
    "        breadth = 4\n",
    "\n",
    "    # Get depth parameter (with default of 2 if input is empty or invalid)\n",
    "    depth_input = ask_question(\"Enter research depth (recommended 1-5, default 2): \")\n",
    "    try:\n",
    "        depth = int(depth_input)\n",
    "    except ValueError:\n",
    "        depth = 2\n",
    "\n",
    "    print(\"Creating research plan...\")\n",
    "    print(initial_query)\n",
    "    print(f\"breadth={breadth}, depth={depth}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a9908fd-c544-4b60-bdd5-e3cc627a4389",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to research? (Enter blank line to finish)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " howto becomemillionaire and make lots of money in a week\n",
      " \n",
      "Enter research breadth (recommended 2-10, default 4):  2\n",
      "Enter research depth (recommended 1-5, default 2):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating research plan...\n",
      "howto becomemillionaire and make lots of money in a week\n",
      "breadth=2, depth=2\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07dab7e-015f-4253-8325-f9e89e643cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m     10\u001b[39m load_dotenv(dotenv_path=\u001b[33m'\u001b[39m\u001b[33m./.env-local\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msystem_prompt_1\u001b[39m():\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import Field, create_model\n",
    "from typing import List\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path='./.env-local')\n",
    "\n",
    "def system_prompt_1():\n",
    "    \"\"\"\n",
    "    Generates a system prompt for API requests, including the current timestamp.\n",
    "    \"\"\"\n",
    "    now = datetime.utcnow().isoformat()\n",
    "    return (\n",
    "        f\"You are an expert researcher. Today is {now}. Follow these instructions when responding:\\n\"\n",
    "        f\"- You may be asked to research subjects that is after your knowledge cutoff, assume the user is right when presented with news.\\n\"\n",
    "        f\"- The user is a highly experienced analyst, no need to simplify it, be as detailed as possible and make sure your response is correct.\\n\"\n",
    "        f\"- Be highly organized.\\n\"\n",
    "        f\"- Suggest solutions that I didn't think about.\\n\"\n",
    "        f\"- Be proactive and anticipate my needs.\\n\"\n",
    "        f\"- Treat me as an expert in all subject matter.\\n\"\n",
    "        f\"- Mistakes erode my trust, so be accurate and thorough.\\n\"\n",
    "        f\"- Provide detailed explanations, I'm comfortable with lots of detail.\\n\"\n",
    "        f\"- Value good arguments over authorities, the source is irrelevant.\\n\"\n",
    "        f\"- Consider new technologies and contrarian ideas, not just the conventional wisdom.\\n\"\n",
    "        f\"- You may use high levels of speculation or prediction, just flag it for me.\"\n",
    "    )\n",
    "\n",
    "# Function to dynamically create JSON Schema Definition using Pydantic\n",
    "def create_followup_model(max_questions: int):\n",
    "    return create_model(\n",
    "        'FollowUpQuestions',\n",
    "        questions=(List[str], Field(description=\"List of follow-up questions.\", max_items=max_questions))\n",
    "    )\n",
    "\n",
    "# Setup the Langchain Chat Model explicitly using GPT-4 JSON mode\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"o3-mini\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.5,\n",
    "    model_kwargs={\"response_format\": {\"type\": \"json_object\", \"reasoning_effort\": \"medium\"}}\n",
    ")\n",
    "\n",
    "# Prompt Templates\n",
    "user_template = \"Given the following query from the user, ask some follow-up questions to clarify the research direction. Return a maximum of {num_questions} questions. User query: {query}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt_1()),\n",
    "    (\"human\", user_template)\n",
    "])\n",
    "\n",
    "# Async function to generate follow-up questions\n",
    "async def generate_feedback(query: str, num_questions: int = 3) -> List[str]:\n",
    "    FollowUpQuestions = create_followup_model(num_questions)\n",
    "    chain = prompt | llm.with_structured_output(FollowUpQuestions)\n",
    "    response = await chain.ainvoke({\"query\": query, \"num_questions\": num_questions})\n",
    "    return response.questions[:num_questions]\n",
    "\n",
    "# Async helper function for multiline user input\n",
    "async def ask_multi_line_question(question: str) -> str:\n",
    "    print(question)\n",
    "    print(\"(Submit an empty line to finish your answer.)\")\n",
    "    lines = []\n",
    "    while True:\n",
    "        line = input()\n",
    "        if not line.strip():\n",
    "            break\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines).strip()\n",
    "\n",
    "# Main script execution\n",
    "async def run():\n",
    "    initial_query = \"can pigs fly\"\n",
    "    breadth = 3\n",
    "    depth = 2\n",
    "\n",
    "    # Generate follow-up questions\n",
    "    follow_up_questions = await generate_feedback(initial_query, breadth)\n",
    "\n",
    "    # Collect user answers\n",
    "    answers = []\n",
    "    for question in follow_up_questions:\n",
    "        answer = await ask_multi_line_question(f\"\\n{question}\\nYour answer:\")\n",
    "        answers.append(answer)\n",
    "\n",
    "    # Combine query with user answers\n",
    "    combined_query = f\"Initial query: {initial_query}\\n\"\n",
    "    combined_query += \"\\n\".join(f\"Q: {q}\\nA: {a}\" for q, a in zip(follow_up_questions, answers))\n",
    "\n",
    "    print(\"\\nPrinting User Prompt with Q & A...\")\n",
    "    print(combined_query)\n",
    "    print(f\"breadth={breadth}, depth={depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "840722a9-65fd-45ee-802f-de61c534471a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e9cdb-beae-4d39-91c0-5cdc296f8e78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 6
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
